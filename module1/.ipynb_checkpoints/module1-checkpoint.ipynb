{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1136d235",
   "metadata": {},
   "source": [
    "# This Module 1 from LLM Zoomcamp from DTC"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f490118d",
   "metadata": {},
   "source": [
    "## LLM Zoomcamp 1.1 - Introduction to LLM and RAG"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97a0b538",
   "metadata": {},
   "source": [
    "### LLM (Large Language Model)\n",
    "\n",
    "- Language Model: Basic language (NLP) models predict next token/word based on previous ones.\n",
    "- LLM: The LMs trained on gorges data with billion and billons of parameter which trained Neural Networks. \n",
    "\n",
    "A Large Language Model (LLM) is a type of artificial intelligence model that uses deep learning to understand and generate human language. It's trained on massive amounts of text data, allowing it to learn patterns and structures in language and perform various natural language processing (NLP) tasks. \n",
    "\n",
    "\n",
    "Here's a more detailed explanation:\n",
    "\n",
    "-  Deep Learning:\n",
    "LLMs are based on deep learning, a subfield of machine learning that uses artificial neural networks with multiple layers to analyze data and learn complex patterns. \n",
    "\n",
    "- Transformer Architecture:\n",
    "Many LLMs are built upon the Transformer architecture, which allows them to process relationships between words in a sentence, even if they're far apart. \n",
    "\n",
    "- Training Data:\n",
    "LLMs are trained on vast amounts of text, such as books, articles, and websites, to learn the nuances of language and its various forms. \n",
    "\n",
    "- Capabilities:\n",
    "LLMs can perform a wide range of NLP tasks, including:\n",
    "    Text Generation: Creating different textual formats, like poems, code, scripts, musical pieces, email, letters, etc. \n",
    "\n",
    "- Translation: Translating languages. \n",
    "    - Question Answering: Answering questions based on provided information. \n",
    "    - Summarization: Condensing large amounts of text into a shorter version. \n",
    "    - Sentiment Analysis: Determining the emotional tone of a piece of text. \n",
    "    - Code Generation: Writing code. \n",
    "\n",
    "- Applications:\n",
    "LLMs have a wide range of applications across various industries, including:\n",
    "    - Customer Service: Providing automated customer support. \n",
    "    - Content Creation: Generating marketing copy, blog posts, and other content. \n",
    "    - Research: Analyzing large datasets of text to extract insights. \n",
    "    - Education: Helping students with writing and language learning. \n",
    "In essence, LLMs are powerful tools that can understand, generate, and manipulate human language, making them valuable in many fields. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff09f3ed",
   "metadata": {},
   "source": [
    "![What is LLM](/module1/LLM-zoomcamp-whatIsLLM.drawio.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08e338d8",
   "metadata": {},
   "source": [
    "### RAG (Retrieval Augmented Generation)\n",
    "\n",
    "Retrieval-Augmented Generation, is a technique in natural language processing (NLP) that combines the strengths of retrieval and generative AI models. It works by first retrieving relevant information from a knowledge base and then using a large language model (LLM) to generate a response that incorporates the retrieved data. This allows for more accurate, up-to-date, and contextually relevant outputs. \n",
    "\n",
    "Here's a more detailed breakdown:\n",
    "\n",
    "*Retrieval*: RAG utilizes search algorithms to query external data sources like databases, knowledge bases, or even the web. \n",
    "\n",
    "*Integration*: The retrieved information is then integrated with a pre-trained LLM. \n",
    "\n",
    "*Generation*: The LLM uses the retrieved data to generate a response, which can be a question answer, a summary, or even new text. \n",
    "\n",
    "Benefits of RAG:\n",
    "- Enhanced Accuracy and Relevance:\n",
    "By accessing external knowledge, RAG can generate more precise and relevant responses. \n",
    "\n",
    "- Improved Contextual Understanding:\n",
    "The retrieved information helps the LLM better understand the context of the user's query, leading to more fitting answers. \n",
    "\n",
    "- Real-time Updates:\n",
    "RAG can incorporate up-to-date information from external sources, ensuring that the generated responses are current. \n",
    "\n",
    "- Source Attributions:\n",
    "RAG can provide citations or references to the sources used to generate the response, improving trust and transparency. \n",
    "\n",
    "- Cost-Effective:\n",
    "RAG can deliver some of the benefits of a custom LLM without the high cost of retraining or fine-tuning a new model. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52cb8ba4",
   "metadata": {},
   "source": [
    "![What is RAG](/module1/LLM-zoomcamp-whatIsRAG.drawio.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c452538f",
   "metadata": {},
   "source": [
    "## LLM Zoomcamp 1.2 - Configuring Your Environment\n",
    "Will be using codespace in loacl vscode v≈üa git."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bc87b77",
   "metadata": {},
   "source": [
    "1. install requierments\n",
    "'''\n",
    "bash \n",
    "pip install tqdm notebook openai elasticsearch pandas scikit-learn ipywidgets\n",
    "'''\n",
    "\n",
    "2. Generate a key in openai and export in terminal\n",
    "'''\n",
    "bash \n",
    "export OPENAI_API_KEY=\"<your key>\"\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e83a2b81",
   "metadata": {},
   "source": [
    "### 1.2: Test openai api"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0787a1bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "api_key = os.getenv(\"OPENAI_API_KEY_V2\") # works in notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bf9a3d35-76d0-4689-a0e8-1dfc4646f91c",
   "metadata": {},
   "outputs": [
    {
     "ename": "OpenAIError",
     "evalue": "The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mOpenAIError\u001b[39m                               Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 4\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mopenai\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m OpenAI\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mos\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m client = \u001b[43mOpenAI\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m      5\u001b[39m \u001b[43m  \u001b[49m\u001b[43mbase_url\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mhttps://openrouter.ai/api/v1\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m      6\u001b[39m \u001b[43m  \u001b[49m\u001b[43mapi_key\u001b[49m\u001b[43m=\u001b[49m\u001b[43mapi_key\u001b[49m\n\u001b[32m      7\u001b[39m \u001b[43m)\u001b[49m\n\u001b[32m      9\u001b[39m completion = client.chat.completions.create(\n\u001b[32m     10\u001b[39m   model=\u001b[33m\"\u001b[39m\u001b[33mmeta-llama/llama-3.3-8b-instruct:free\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m     11\u001b[39m   messages=[\n\u001b[32m   (...)\u001b[39m\u001b[32m     16\u001b[39m   ]\n\u001b[32m     17\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/python/3.12.1/lib/python3.12/site-packages/openai/_client.py:126\u001b[39m, in \u001b[36mOpenAI.__init__\u001b[39m\u001b[34m(self, api_key, organization, project, base_url, websocket_base_url, timeout, max_retries, default_headers, default_query, http_client, _strict_response_validation)\u001b[39m\n\u001b[32m    124\u001b[39m     api_key = os.environ.get(\u001b[33m\"\u001b[39m\u001b[33mOPENAI_API_KEY\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    125\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m api_key \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m126\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m OpenAIError(\n\u001b[32m    127\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mThe api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    128\u001b[39m     )\n\u001b[32m    129\u001b[39m \u001b[38;5;28mself\u001b[39m.api_key = api_key\n\u001b[32m    131\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m organization \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[31mOpenAIError\u001b[39m: The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable"
     ]
    }
   ],
   "source": [
    "from openai import OpenAI\n",
    "import os\n",
    "\n",
    "client = OpenAI(\n",
    "  base_url=\"https://openrouter.ai/api/v1\",\n",
    "  api_key=api_key\n",
    ")\n",
    "\n",
    "completion = client.chat.completions.create(\n",
    "  model=\"meta-llama/llama-3.3-8b-instruct:free\",\n",
    "  messages=[\n",
    "    {\n",
    "      \"role\": \"user\",\n",
    "      \"content\": \"What is the meaning of life?\"\n",
    "    }\n",
    "  ]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d2a422d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(completion.choices[0].message.content)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f7a8f0fd",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'client' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m completion = \u001b[43mclient\u001b[49m.chat.completions.create(\n\u001b[32m      2\u001b[39m   model=\u001b[33m\"\u001b[39m\u001b[33mmeta-llama/llama-3.3-8b-instruct:free\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m      3\u001b[39m   messages=[\n\u001b[32m      4\u001b[39m     {\n\u001b[32m      5\u001b[39m       \u001b[33m\"\u001b[39m\u001b[33mrole\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33muser\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m      6\u001b[39m       \u001b[33m\"\u001b[39m\u001b[33mcontent\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33mIs it toolate to join the course?\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m      7\u001b[39m     }\n\u001b[32m      8\u001b[39m   ]\n\u001b[32m      9\u001b[39m )\n\u001b[32m     11\u001b[39m \u001b[38;5;28mprint\u001b[39m(completion.choices[\u001b[32m0\u001b[39m].message.content)\n",
      "\u001b[31mNameError\u001b[39m: name 'client' is not defined"
     ]
    }
   ],
   "source": [
    "completion = client.chat.completions.create(\n",
    "  model=\"meta-llama/llama-3.3-8b-instruct:free\",\n",
    "  messages=[\n",
    "    {\n",
    "      \"role\": \"user\",\n",
    "      \"content\": \"Is it toolate to join the course?\"\n",
    "    }\n",
    "  ]\n",
    ")\n",
    "\n",
    "print(completion.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2911a270",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
