{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "27126f05",
   "metadata": {},
   "source": [
    "## Homework: Agents\n",
    "\n",
    "In this homework, we will learn more about function calling,\n",
    "and we will also explore MCP - model-context protocol. \n",
    "\n",
    "\n",
    "## Preparation\n",
    "\n",
    "First, we'll define a function that we will use when building\n",
    "our agent. \n",
    "\n",
    "It will generate fake weather data:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "72d6a69d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "known_weather_data = {\n",
    "    'berlin': 20.0\n",
    "}\n",
    "\n",
    "def get_weather(city: str) -> float:\n",
    "    city = city.strip().lower()\n",
    "\n",
    "    if city in known_weather_data:\n",
    "        return known_weather_data[city]\n",
    "\n",
    "    return round(random.uniform(-5, 35), 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f22f8e7d",
   "metadata": {},
   "source": [
    "## Q1. Define function description\n",
    "\n",
    "We want to use it as a tool for our agent, so we need to \n",
    "describe it \n",
    "\n",
    "How should the description for this function look like? Fill in missing parts\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b8e6148",
   "metadata": {},
   "source": [
    "get_weather_tool = {\n",
    "    \"type\": \"function\",\n",
    "    \"name\": \"get_weather\",\n",
    "    \"description\": \"Retrieve the temperature for a specific city\",\n",
    "    \"parameters\": {\n",
    "        \"type\": \"object\",\n",
    "        \"properties\": {\n",
    "            \"city\": {\n",
    "                \"type\": \"string\",\n",
    "                \"description\": \"Name of the city to get weather data for\"\n",
    "            }\n",
    "        },\n",
    "        \"required\": [\"city\"],\n",
    "        \"additionalProperties\": False\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "510f51c8",
   "metadata": {},
   "source": [
    "Answers:\n",
    "\n",
    "- TODO1 â†’ \"get_weather\"  â† matches the function name\n",
    "- TODO2 â†’ \"Retrieve the temperature for a specific city\"\n",
    "- TODO3 â†’ \"city\"\n",
    "- TODO4 â†’ \"Name of the city to get weather data for\"\n",
    "- TODO5 â†’ \"city\" (as a string in a list: [\"city\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf77ec34",
   "metadata": {},
   "source": [
    "## Testing it (Optional)\n",
    "\n",
    "If you have OpenAI API Key (or alternative provider),\n",
    "let's test it.\n",
    "\n",
    "A question could be \"What's the weather like in Germany?\"\n",
    "\n",
    "Experiment with different system prompts to have better answers\n",
    "from the system.\n",
    "\n",
    "You can use [chat_assistant.py](https://github.com/alexeygrigorev/rag-agents-workshop/blob/main/chat_assistant.py)\n",
    "or implement everything yourself "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61f779ab",
   "metadata": {},
   "source": [
    "## Q2. Adding another tool\n",
    "\n",
    "Let's add another tool - a function that can add weather data\n",
    "to our database:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0ea07a2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_weather(city: str, temp: float) -> None:\n",
    "    city = city.strip().lower()\n",
    "    known_weather_data[city] = temp\n",
    "    return 'OK'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44eea740",
   "metadata": {},
   "source": [
    "Now let's write a description for it.\n",
    "\n",
    "What did you write?\n",
    "\n",
    "Optionally, you can test it after adding this function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2f6ab02e",
   "metadata": {},
   "outputs": [],
   "source": [
    "set_weather_tool = {\n",
    "    \"type\": \"function\",\n",
    "    \"name\": \"set_weather\",\n",
    "    \"description\": \"Set the temperature for a specific city\",\n",
    "    \"parameters\": {\n",
    "        \"type\": \"object\",\n",
    "        \"properties\": {\n",
    "            \"city\": {\n",
    "                \"type\": \"string\",\n",
    "                \"description\": \"Name of the city to set weather data for\"\n",
    "            },\n",
    "            \"temp\": {\n",
    "                \"type\": \"number\",\n",
    "                \"description\": \"Temperature to associate with the city\"\n",
    "            }\n",
    "        },\n",
    "        \"required\": [\"city\", \"temp\"],\n",
    "        \"additionalProperties\": False\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84c9bbd4",
   "metadata": {},
   "source": [
    "## MCP\n",
    "\n",
    "MCP stands for Model-Context Protocol. It allows LLMs communicate\n",
    "with different tools (like Qdrant). It's function calling, but \n",
    "one step further:\n",
    "\n",
    "* A tool can export a list of functions it has\n",
    "* When we include the tool to our Agent, we just need to include the link to the MCP server"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39f97c05",
   "metadata": {},
   "source": [
    "## Q3. Install FastMCP\n",
    "\n",
    "Let's install a library for MCP - [FastMCP](https://github.com/jlowin/fastmcp):\n",
    "\n",
    "```bash\n",
    "pip install fastmcp\n",
    "```\n",
    "\n",
    "What's the version of FastMCP you installed?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2653c0b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fastmcp                   2.10.5\n"
     ]
    }
   ],
   "source": [
    "!pip list |grep fastmcp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1aca9aa",
   "metadata": {},
   "source": [
    "## Q4. Simple MCP Server \n",
    "\n",
    "A simple MCP server from the documentation looks like that:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0b1b6e6",
   "metadata": {},
   "source": [
    "In our case, we need to write docstrings for our functions.\n",
    "\n",
    "Let's ask ChatGPT for help:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "07d27897",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[2mâ•­â”€\u001b[0m\u001b[2m FastMCP 2.0 \u001b[0m\u001b[2mâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\u001b[0m\u001b[2mâ”€â•®\u001b[0m\n",
      "\u001b[2mâ”‚\u001b[0m                                                                            \u001b[2mâ”‚\u001b[0m\n",
      "\u001b[2mâ”‚\u001b[0m    \u001b[1;32m    _ __ ___ ______           __  __  _____________    ____    ____ \u001b[0m    \u001b[2mâ”‚\u001b[0m\n",
      "\u001b[2mâ”‚\u001b[0m    \u001b[1;32m   _ __ ___ / ____/___ ______/ /_/  |/  / ____/ __ \\  |___ \\  / __ \\\u001b[0m    \u001b[2mâ”‚\u001b[0m\n",
      "\u001b[2mâ”‚\u001b[0m    \u001b[1;32m  _ __ ___ / /_  / __ `/ ___/ __/ /|_/ / /   / /_/ /  ___/ / / / / /\u001b[0m    \u001b[2mâ”‚\u001b[0m\n",
      "\u001b[2mâ”‚\u001b[0m    \u001b[1;32m _ __ ___ / __/ / /_/ (__  ) /_/ /  / / /___/ ____/  /  __/_/ /_/ / \u001b[0m    \u001b[2mâ”‚\u001b[0m\n",
      "\u001b[2mâ”‚\u001b[0m    \u001b[1;32m_ __ ___ /_/    \\__,_/____/\\__/_/  /_/\\____/_/      /_____(_)____/  \u001b[0m    \u001b[2mâ”‚\u001b[0m\n",
      "\u001b[2mâ”‚\u001b[0m                                                                            \u001b[2mâ”‚\u001b[0m\n",
      "\u001b[2mâ”‚\u001b[0m                                                                            \u001b[2mâ”‚\u001b[0m\n",
      "\u001b[2mâ”‚\u001b[0m                                                                            \u001b[2mâ”‚\u001b[0m\n",
      "\u001b[2mâ”‚\u001b[0m    \u001b[1mğŸ–¥ï¸ \u001b[0m\u001b[1m \u001b[0m\u001b[1;36mServer name:    \u001b[0m\u001b[1;36m \u001b[0m\u001b[37mWeather Server â˜ï¸     \u001b[0m                               \u001b[2mâ”‚\u001b[0m\n",
      "\u001b[2mâ”‚\u001b[0m    \u001b[1mğŸ“¦\u001b[0m\u001b[1m \u001b[0m\u001b[1;36mTransport:      \u001b[0m\u001b[1;36m \u001b[0m\u001b[37mSTDIO                \u001b[0m                               \u001b[2mâ”‚\u001b[0m\n",
      "\u001b[2mâ”‚\u001b[0m    \u001b[1m  \u001b[0m\u001b[1m \u001b[0m\u001b[1;36m                \u001b[0m\u001b[1;36m \u001b[0m\u001b[37m                     \u001b[0m                               \u001b[2mâ”‚\u001b[0m\n",
      "\u001b[2mâ”‚\u001b[0m    \u001b[1mğŸ“š\u001b[0m\u001b[1m \u001b[0m\u001b[1;36mDocs:           \u001b[0m\u001b[1;36m \u001b[0m\u001b[37mhttps://gofastmcp.com\u001b[0m                               \u001b[2mâ”‚\u001b[0m\n",
      "\u001b[2mâ”‚\u001b[0m    \u001b[1mğŸš€\u001b[0m\u001b[1m \u001b[0m\u001b[1;36mDeploy:         \u001b[0m\u001b[1;36m \u001b[0m\u001b[37mhttps://fastmcp.cloud\u001b[0m                               \u001b[2mâ”‚\u001b[0m\n",
      "\u001b[2mâ”‚\u001b[0m    \u001b[1m  \u001b[0m\u001b[1m \u001b[0m\u001b[1;36m                \u001b[0m\u001b[1;36m \u001b[0m\u001b[37m                     \u001b[0m                               \u001b[2mâ”‚\u001b[0m\n",
      "\u001b[2mâ”‚\u001b[0m    \u001b[1mğŸï¸ \u001b[0m\u001b[1m \u001b[0m\u001b[1;36mFastMCP version:\u001b[0m\u001b[1;36m \u001b[0m\u001b[2;37m2.10.5               \u001b[0m                               \u001b[2mâ”‚\u001b[0m\n",
      "\u001b[2mâ”‚\u001b[0m    \u001b[1mğŸ¤\u001b[0m\u001b[1m \u001b[0m\u001b[1;36mMCP version:    \u001b[0m\u001b[1;36m \u001b[0m\u001b[2;37m1.12.0               \u001b[0m                               \u001b[2mâ”‚\u001b[0m\n",
      "\u001b[2mâ”‚\u001b[0m                                                                            \u001b[2mâ”‚\u001b[0m\n",
      "\u001b[2mâ•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[2;36m[07/19/25 23:11:09]\u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m Starting MCP server \u001b[32m'Weather Server \u001b[0m \u001b]8;id=267177;file:///home/codespace/miniconda3/lib/python3.13/site-packages/fastmcp/server/server.py\u001b\\\u001b[2mserver.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=932844;file:///home/codespace/miniconda3/lib/python3.13/site-packages/fastmcp/server/server.py#1371\u001b\\\u001b[2m1371\u001b[0m\u001b]8;;\u001b\\\n",
      "\u001b[2;36m                    \u001b[0m         \u001b[32mâ˜ï¸'\u001b[0m with transport \u001b[32m'stdio'\u001b[0m            \u001b[2m              \u001b[0m\n",
      "^C\n",
      "Traceback (most recent call last):\n",
      "  File \u001b[35m\"/workspaces/LLM-Zoomcamp/module3/weather_server.py\"\u001b[0m, line \u001b[35m24\u001b[0m, in \u001b[35m<module>\u001b[0m\n",
      "    \u001b[31mmcp.run\u001b[0m\u001b[1;31m()\u001b[0m\n",
      "    \u001b[31m~~~~~~~\u001b[0m\u001b[1;31m^^\u001b[0m\n",
      "  File \u001b[35m\"/home/codespace/miniconda3/lib/python3.13/site-packages/fastmcp/server/server.py\"\u001b[0m, line \u001b[35m328\u001b[0m, in \u001b[35mrun\u001b[0m\n",
      "    \u001b[31manyio.run\u001b[0m\u001b[1;31m(\u001b[0m\n",
      "    \u001b[31m~~~~~~~~~\u001b[0m\u001b[1;31m^\u001b[0m\n",
      "        \u001b[1;31mpartial(\u001b[0m\n",
      "        \u001b[1;31m^^^^^^^^\u001b[0m\n",
      "    ...<4 lines>...\n",
      "        \u001b[1;31m)\u001b[0m\n",
      "        \u001b[1;31m^\u001b[0m\n",
      "    \u001b[1;31m)\u001b[0m\n",
      "    \u001b[1;31m^\u001b[0m\n",
      "  File \u001b[35m\"/home/codespace/miniconda3/lib/python3.13/site-packages/anyio/_core/_eventloop.py\"\u001b[0m, line \u001b[35m74\u001b[0m, in \u001b[35mrun\u001b[0m\n",
      "    return \u001b[31masync_backend.run\u001b[0m\u001b[1;31m(func, args, {}, backend_options)\u001b[0m\n",
      "           \u001b[31m~~~~~~~~~~~~~~~~~\u001b[0m\u001b[1;31m^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[0m\n",
      "  File \u001b[35m\"/home/codespace/miniconda3/lib/python3.13/site-packages/anyio/_backends/_asyncio.py\"\u001b[0m, line \u001b[35m2310\u001b[0m, in \u001b[35mrun\u001b[0m\n",
      "    return \u001b[31mrunner.run\u001b[0m\u001b[1;31m(wrapper())\u001b[0m\n",
      "           \u001b[31m~~~~~~~~~~\u001b[0m\u001b[1;31m^^^^^^^^^^^\u001b[0m\n",
      "  File \u001b[35m\"/home/codespace/miniconda3/lib/python3.13/asyncio/runners.py\"\u001b[0m, line \u001b[35m118\u001b[0m, in \u001b[35mrun\u001b[0m\n",
      "    return \u001b[31mself._loop.run_until_complete\u001b[0m\u001b[1;31m(task)\u001b[0m\n",
      "           \u001b[31m~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\u001b[0m\u001b[1;31m^^^^^^\u001b[0m\n",
      "  File \u001b[35m\"/home/codespace/miniconda3/lib/python3.13/asyncio/base_events.py\"\u001b[0m, line \u001b[35m712\u001b[0m, in \u001b[35mrun_until_complete\u001b[0m\n",
      "    \u001b[31mself.run_forever\u001b[0m\u001b[1;31m()\u001b[0m\n",
      "    \u001b[31m~~~~~~~~~~~~~~~~\u001b[0m\u001b[1;31m^^\u001b[0m\n",
      "  File \u001b[35m\"/home/codespace/miniconda3/lib/python3.13/asyncio/base_events.py\"\u001b[0m, line \u001b[35m683\u001b[0m, in \u001b[35mrun_forever\u001b[0m\n",
      "    \u001b[31mself._run_once\u001b[0m\u001b[1;31m()\u001b[0m\n",
      "    \u001b[31m~~~~~~~~~~~~~~\u001b[0m\u001b[1;31m^^\u001b[0m\n",
      "  File \u001b[35m\"/home/codespace/miniconda3/lib/python3.13/asyncio/base_events.py\"\u001b[0m, line \u001b[35m2002\u001b[0m, in \u001b[35m_run_once\u001b[0m\n",
      "    event_list = self._selector.select(timeout)\n",
      "  File \u001b[35m\"/home/codespace/miniconda3/lib/python3.13/selectors.py\"\u001b[0m, line \u001b[35m452\u001b[0m, in \u001b[35mselect\u001b[0m\n",
      "    fd_event_list = self._selector.poll(timeout, max_ev)\n",
      "  File \u001b[35m\"/home/codespace/miniconda3/lib/python3.13/asyncio/runners.py\"\u001b[0m, line \u001b[35m157\u001b[0m, in \u001b[35m_on_sigint\u001b[0m\n",
      "    raise KeyboardInterrupt()\n",
      "\u001b[1;35mKeyboardInterrupt\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!python weather_server.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "184ba562",
   "metadata": {},
   "source": [
    "So, for 'TODO' the answer is:\n",
    "\n",
    "'stdio'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3428473d",
   "metadata": {},
   "source": [
    "## Q5. Protocol\n",
    "\n",
    "There are different ways to communicate with an MCP server.\n",
    "Ours is currently running using standart input/output, which\n",
    "means that the client write something to stdin and read the\n",
    "answer using stdout.\n",
    "\n",
    "Our weather server is currently running.\n",
    "\n",
    "This is how we start communitcating with it:\n",
    "\n",
    "- First, we send an initialization request -- this way, we register our client with the server:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "db09cc3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# json_rpc_test.py\n",
    "import subprocess\n",
    "import json\n",
    "import time\n",
    "\n",
    "# Start the server process\n",
    "process = subprocess.Popen(\n",
    "    [\"python\", \"weather_server.py\"],\n",
    "    stdin=subprocess.PIPE,\n",
    "    stdout=subprocess.PIPE,\n",
    "    stderr=subprocess.PIPE,\n",
    "    text=True\n",
    ")\n",
    "\n",
    "def send_msg(obj):\n",
    "    json_str = json.dumps(obj) + '\\n'\n",
    "    process.stdin.write(json_str)\n",
    "    process.stdin.flush()\n",
    "    print(\">> Sent:\", json_str.strip())\n",
    "\n",
    "\n",
    "def read_response():\n",
    "    line = process.stdout.readline()\n",
    "    if line:\n",
    "        print(\"<< Received:\", line.strip())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4b2b7c6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">> Sent: {\"jsonrpc\": \"2.0\", \"id\": 1, \"method\": \"initialize\", \"params\": {\"protocolVersion\": \"2024-11-05\", \"capabilities\": {\"roots\": {\"listChanged\": true}, \"sampling\": {}}, \"clientInfo\": {\"name\": \"test-client\", \"version\": \"1.0.0\"}}}\n",
      "<< Received: {\"jsonrpc\":\"2.0\",\"id\":1,\"result\":{\"protocolVersion\":\"2024-11-05\",\"capabilities\":{\"experimental\":{},\"prompts\":{\"listChanged\":false},\"resources\":{\"subscribe\":false,\"listChanged\":false},\"tools\":{\"listChanged\":true}},\"serverInfo\":{\"name\":\"Weather Server â˜ï¸\",\"version\":\"1.12.0\"}}}\n"
     ]
    }
   ],
   "source": [
    "# Step 1: Initialize\n",
    "send_msg({\n",
    "    \"jsonrpc\": \"2.0\",\n",
    "    \"id\": 1,\n",
    "    \"method\": \"initialize\",\n",
    "    \"params\": {\n",
    "        \"protocolVersion\": \"2024-11-05\",\n",
    "        \"capabilities\": {\n",
    "            \"roots\": {\"listChanged\": True},\n",
    "            \"sampling\": {}\n",
    "        },\n",
    "        \"clientInfo\": {\n",
    "            \"name\": \"test-client\",\n",
    "            \"version\": \"1.0.0\"\n",
    "        }\n",
    "    }\n",
    "})\n",
    "read_response()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b1ec6325",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">> Sent: {\"jsonrpc\": \"2.0\", \"method\": \"notifications/initialized\"}\n",
      ">> Sent: {\"jsonrpc\": \"2.0\", \"id\": 2, \"method\": \"tools/list\"}\n",
      "<< Received: {\"jsonrpc\":\"2.0\",\"id\":2,\"result\":{\"tools\":[{\"name\":\"get_weather\",\"description\":\"Retrieve the temperature for a specific city.\",\"inputSchema\":{\"properties\":{\"city\":{\"title\":\"City\",\"type\":\"string\"}},\"required\":[\"city\"],\"type\":\"object\"},\"outputSchema\":{\"properties\":{\"result\":{\"title\":\"Result\",\"type\":\"number\"}},\"required\":[\"result\"],\"title\":\"_WrappedResult\",\"type\":\"object\",\"x-fastmcp-wrap-result\":true}},{\"name\":\"set_weather\",\"description\":\"Set the temperature for a specific city.\",\"inputSchema\":{\"properties\":{\"city\":{\"title\":\"City\",\"type\":\"string\"},\"temp\":{\"title\":\"Temp\",\"type\":\"number\"}},\"required\":[\"city\",\"temp\"],\"type\":\"object\"},\"outputSchema\":{\"properties\":{\"result\":{\"title\":\"Result\",\"type\":\"string\"}},\"required\":[\"result\"],\"title\":\"_WrappedResult\",\"type\":\"object\",\"x-fastmcp-wrap-result\":true}}]}}\n"
     ]
    }
   ],
   "source": [
    "# Step 2: Initialized\n",
    "send_msg({\"jsonrpc\": \"2.0\", \"method\": \"notifications/initialized\"})\n",
    "time.sleep(0.1)  # Give server time\n",
    "\n",
    "# Step 3: List tools\n",
    "send_msg({\"jsonrpc\": \"2.0\", \"id\": 2, \"method\": \"tools/list\"})\n",
    "read_response()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d9b35ae3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">> Sent: {\"jsonrpc\": \"2.0\", \"id\": 3, \"method\": \"tools/call\", \"params\": {\"name\": \"get_weather\", \"arguments\": {\"city\": \"Berlin\"}}}\n",
      "<< Received: {\"jsonrpc\":\"2.0\",\"id\":3,\"result\":{\"content\":[{\"type\":\"text\",\"text\":\"20.0\"}],\"structuredContent\":{\"result\":20.0},\"isError\":false}}\n"
     ]
    }
   ],
   "source": [
    "# Step 4: Call get_weather\n",
    "send_msg({\n",
    "    \"jsonrpc\": \"2.0\",\n",
    "    \"id\": 3,\n",
    "    \"method\": \"tools/call\",\n",
    "    \"params\": {\n",
    "        \"name\": \"get_weather\",\n",
    "        \"arguments\": {\"city\": \"Berlin\"}\n",
    "    }\n",
    "})\n",
    "read_response()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a5311aa",
   "metadata": {},
   "source": [
    "- What did you get in response?\n",
    "\n",
    "20"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b031e66b",
   "metadata": {},
   "source": [
    "## Q6. Client\n",
    "\n",
    "We typically don't interact with the server by copy-pasting \n",
    "commands in the terminal.\n",
    "\n",
    "In practice, we use an MCP Client. Let's implement it. \n",
    "\n",
    "FastMCP also supports MCP clients:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e7fbde8",
   "metadata": {},
   "source": [
    "Use the client to get the list of available tools\n",
    "of our script. How does the result look like?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ba95da3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available tools:\n",
      "name='get_weather' title=None description='Retrieve the temperature for a specific city.' inputSchema={'properties': {'city': {'title': 'City', 'type': 'string'}}, 'required': ['city'], 'type': 'object'} outputSchema={'properties': {'result': {'title': 'Result', 'type': 'number'}}, 'required': ['result'], 'title': '_WrappedResult', 'type': 'object', 'x-fastmcp-wrap-result': True} annotations=None meta=None\n",
      "name='set_weather' title=None description='Set the temperature for a specific city.' inputSchema={'properties': {'city': {'title': 'City', 'type': 'string'}, 'temp': {'title': 'Temp', 'type': 'number'}}, 'required': ['city', 'temp'], 'type': 'object'} outputSchema={'properties': {'result': {'title': 'Result', 'type': 'string'}}, 'required': ['result'], 'title': '_WrappedResult', 'type': 'object', 'x-fastmcp-wrap-result': True} annotations=None meta=None\n"
     ]
    }
   ],
   "source": [
    "from fastmcp import Client\n",
    "\n",
    "import weather_server\n",
    "async with Client(weather_server.mcp) as mcp_client:\n",
    "    tools = await mcp_client.list_tools()\n",
    "    print(\"Available tools:\")\n",
    "    for tool in tools:\n",
    "        print(tool)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "324fe7ed",
   "metadata": {},
   "source": [
    "## Using tools from the MCP server (optional)\n",
    "\n",
    "FastMCP uses asyncio for client-server communication. In our\n",
    "case, the code we wrote previously in the module\n",
    "(chat_assistant.py) is not asyncio-friendly, so it will\n",
    "require a lot of adjustments to run it. \n",
    "\n",
    "Which is why we asked Claude to implement a simple\n",
    "non-async MCP client (see [mcp_client.py](mcp_client.py))\n",
    "that can only do this:\n",
    "\n",
    "- List tools\n",
    "- Invoke the specified tool\n",
    "\n",
    "Note: this is not a production-ready MCP Client! Use it\n",
    "only for learning purposes.\n",
    "\n",
    "Check the code - it's quite illustrative. Or experiment\n",
    "with writing this code yourself.\n",
    "\n",
    "Here's how we can use it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2a819943",
   "metadata": {},
   "outputs": [],
   "source": [
    "import mcp_client\n",
    "\n",
    "our_mcp_client = mcp_client.MCPClient([\"python\", \"weather_server.py\"])\n",
    "our_mcp_client.start_server()\n",
    "our_mcp_client.initialize()\n",
    "our_mcp_client.initialized()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2bdebdcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "class MCPTools:\n",
    "    def __init__(self, mcp_client):\n",
    "        self.mcp_client = mcp_client\n",
    "        self.tools = None\n",
    "\n",
    "    def get_tools(self):\n",
    "        if self.tools is None:\n",
    "            mcp_response = self.mcp_client.get_tools()\n",
    "\n",
    "            if isinstance(mcp_response, dict) and \"tools\" in mcp_response:\n",
    "                mcp_tools = mcp_response[\"tools\"]\n",
    "            else:\n",
    "                raise ValueError(\"Unexpected response format from MCP client\")\n",
    "\n",
    "            self.tools = convert_tools_list(mcp_tools)\n",
    "\n",
    "        return self.tools\n",
    "\n",
    "    def function_call(self, tool_call_response):\n",
    "        function_name = tool_call_response[\"name\"]\n",
    "        arguments = json.loads(tool_call_response[\"arguments\"])\n",
    "        result = self.mcp_client.call_tool(function_name, arguments)\n",
    "\n",
    "        return {\n",
    "            \"type\": \"function_call_output\",\n",
    "            \"call_id\": tool_call_response[\"call_id\"],\n",
    "            \"output\": json.dumps(result, indent=2),\n",
    "        }\n",
    "\n",
    "def convert_tools_list(tools):\n",
    "    return [\n",
    "        {\n",
    "            \"type\": \"function\",\n",
    "            \"function\": {\n",
    "                \"name\": tool[\"name\"],\n",
    "                \"description\": tool[\"description\"],\n",
    "                \"parameters\": tool[\"inputSchema\"]\n",
    "            }\n",
    "        }\n",
    "        for tool in tools\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3a2c7343",
   "metadata": {},
   "outputs": [],
   "source": [
    "developer_prompt = \"\"\"\n",
    "You help users find out the weather in their cities.\n",
    "If they didn't specify a city, ask them. Make sure we always use a city.\n",
    "\"\"\".strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "da676989",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from openai import OpenAI\n",
    "\n",
    "token = os.environ[\"GITHUB_TOKEN\"]\n",
    "endpoint = \"https://models.github.ai/inference\"\n",
    "model_name = \"openai/gpt-4o\"\n",
    "\n",
    "client = OpenAI(\n",
    "    base_url=endpoint,\n",
    "    api_key=token,\n",
    ")\n",
    "\n",
    "# #test client\n",
    "\n",
    "# response = client.chat.completions.create(\n",
    "#     messages=[\n",
    "#         {\n",
    "#             \"role\": \"system\",\n",
    "#             \"content\": \"You are a helpful assistant.\",\n",
    "#         },\n",
    "#         {\n",
    "#             \"role\": \"user\",\n",
    "#             \"content\": \"What is the capital of France?\",\n",
    "#         }\n",
    "#     ],\n",
    "#     temperature=1.0,\n",
    "#     top_p=1.0,\n",
    "#     max_tokens=1000,\n",
    "#     model=model_name\n",
    "# )\n",
    "\n",
    "# print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "850c0b50",
   "metadata": {},
   "outputs": [
    {
     "ename": "RateLimitError",
     "evalue": "Error code: 429 - {'error': {'code': 'RateLimitReached', 'message': 'Rate limit of 50 per 86400s exceeded for UserByModelByDay. Please wait 43414 seconds before retrying.', 'details': 'Rate limit of 50 per 86400s exceeded for UserByModelByDay. Please wait 43414 seconds before retrying.'}}",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRateLimitError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[15]\u001b[39m\u001b[32m, line 10\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mchat_assistant\u001b[39;00m\n\u001b[32m      3\u001b[39m chat = chat_assistant.ChatAssistant(\n\u001b[32m      4\u001b[39m     tools=MCPTools(mcp_client=our_mcp_client),\n\u001b[32m      5\u001b[39m     developer_prompt=developer_prompt,\n\u001b[32m      6\u001b[39m     chat_interface=chat_assistant.ChatInterface(),\n\u001b[32m      7\u001b[39m     client=client\n\u001b[32m      8\u001b[39m )\n\u001b[32m---> \u001b[39m\u001b[32m10\u001b[39m \u001b[43mchat\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/workspaces/LLM-Zoomcamp/module3/chat_assistant.py:97\u001b[39m, in \u001b[36mChatAssistant.run\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m     94\u001b[39m chat_messages.append(user_message)\n\u001b[32m     96\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m97\u001b[39m     response = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mgpt\u001b[49m\u001b[43m(\u001b[49m\u001b[43mchat_messages\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     98\u001b[39m     choice = response.choices[\u001b[32m0\u001b[39m]\n\u001b[32m     99\u001b[39m     message = choice.message\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/workspaces/LLM-Zoomcamp/module3/chat_assistant.py:78\u001b[39m, in \u001b[36mChatAssistant.gpt\u001b[39m\u001b[34m(self, chat_messages)\u001b[39m\n\u001b[32m     77\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mgpt\u001b[39m(\u001b[38;5;28mself\u001b[39m, chat_messages):\n\u001b[32m---> \u001b[39m\u001b[32m78\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mclient\u001b[49m\u001b[43m.\u001b[49m\u001b[43mchat\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcompletions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcreate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     79\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mgpt-4o\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     80\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m=\u001b[49m\u001b[43mchat_messages\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     81\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtools\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtools\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_tools\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     82\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/lib/python3.13/site-packages/openai/_utils/_utils.py:287\u001b[39m, in \u001b[36mrequired_args.<locals>.inner.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    285\u001b[39m             msg = \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mMissing required argument: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mquote(missing[\u001b[32m0\u001b[39m])\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    286\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(msg)\n\u001b[32m--> \u001b[39m\u001b[32m287\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/lib/python3.13/site-packages/openai/resources/chat/completions/completions.py:925\u001b[39m, in \u001b[36mCompletions.create\u001b[39m\u001b[34m(self, messages, model, audio, frequency_penalty, function_call, functions, logit_bias, logprobs, max_completion_tokens, max_tokens, metadata, modalities, n, parallel_tool_calls, prediction, presence_penalty, reasoning_effort, response_format, seed, service_tier, stop, store, stream, stream_options, temperature, tool_choice, tools, top_logprobs, top_p, user, web_search_options, extra_headers, extra_query, extra_body, timeout)\u001b[39m\n\u001b[32m    882\u001b[39m \u001b[38;5;129m@required_args\u001b[39m([\u001b[33m\"\u001b[39m\u001b[33mmessages\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mmodel\u001b[39m\u001b[33m\"\u001b[39m], [\u001b[33m\"\u001b[39m\u001b[33mmessages\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mmodel\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mstream\u001b[39m\u001b[33m\"\u001b[39m])\n\u001b[32m    883\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mcreate\u001b[39m(\n\u001b[32m    884\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m    922\u001b[39m     timeout: \u001b[38;5;28mfloat\u001b[39m | httpx.Timeout | \u001b[38;5;28;01mNone\u001b[39;00m | NotGiven = NOT_GIVEN,\n\u001b[32m    923\u001b[39m ) -> ChatCompletion | Stream[ChatCompletionChunk]:\n\u001b[32m    924\u001b[39m     validate_response_format(response_format)\n\u001b[32m--> \u001b[39m\u001b[32m925\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_post\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    926\u001b[39m \u001b[43m        \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m/chat/completions\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    927\u001b[39m \u001b[43m        \u001b[49m\u001b[43mbody\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmaybe_transform\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    928\u001b[39m \u001b[43m            \u001b[49m\u001b[43m{\u001b[49m\n\u001b[32m    929\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmessages\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    930\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmodel\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    931\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43maudio\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43maudio\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    932\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mfrequency_penalty\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfrequency_penalty\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    933\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mfunction_call\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunction_call\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    934\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mfunctions\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunctions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    935\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mlogit_bias\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogit_bias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    936\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mlogprobs\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogprobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    937\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmax_completion_tokens\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_completion_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    938\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmax_tokens\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    939\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmetadata\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    940\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmodalities\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodalities\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    941\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mn\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    942\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mparallel_tool_calls\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mparallel_tool_calls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    943\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mprediction\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mprediction\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    944\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mpresence_penalty\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mpresence_penalty\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    945\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mreasoning_effort\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mreasoning_effort\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    946\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mresponse_format\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mresponse_format\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    947\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mseed\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mseed\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    948\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mservice_tier\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mservice_tier\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    949\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstop\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    950\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstore\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstore\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    951\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstream\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    952\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstream_options\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    953\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtemperature\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    954\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtool_choice\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtool_choice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    955\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtools\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtools\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    956\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtop_logprobs\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop_logprobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    957\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtop_p\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop_p\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    958\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43muser\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43muser\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    959\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mweb_search_options\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mweb_search_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    960\u001b[39m \u001b[43m            \u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    961\u001b[39m \u001b[43m            \u001b[49m\u001b[43mcompletion_create_params\u001b[49m\u001b[43m.\u001b[49m\u001b[43mCompletionCreateParamsStreaming\u001b[49m\n\u001b[32m    962\u001b[39m \u001b[43m            \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\n\u001b[32m    963\u001b[39m \u001b[43m            \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mcompletion_create_params\u001b[49m\u001b[43m.\u001b[49m\u001b[43mCompletionCreateParamsNonStreaming\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    964\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    965\u001b[39m \u001b[43m        \u001b[49m\u001b[43moptions\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmake_request_options\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    966\u001b[39m \u001b[43m            \u001b[49m\u001b[43mextra_headers\u001b[49m\u001b[43m=\u001b[49m\u001b[43mextra_headers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mextra_query\u001b[49m\u001b[43m=\u001b[49m\u001b[43mextra_query\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mextra_body\u001b[49m\u001b[43m=\u001b[49m\u001b[43mextra_body\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout\u001b[49m\n\u001b[32m    967\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    968\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[43m=\u001b[49m\u001b[43mChatCompletion\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    969\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstream\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    970\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[43m=\u001b[49m\u001b[43mStream\u001b[49m\u001b[43m[\u001b[49m\u001b[43mChatCompletionChunk\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    971\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/lib/python3.13/site-packages/openai/_base_client.py:1249\u001b[39m, in \u001b[36mSyncAPIClient.post\u001b[39m\u001b[34m(self, path, cast_to, body, options, files, stream, stream_cls)\u001b[39m\n\u001b[32m   1235\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mpost\u001b[39m(\n\u001b[32m   1236\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   1237\u001b[39m     path: \u001b[38;5;28mstr\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1244\u001b[39m     stream_cls: \u001b[38;5;28mtype\u001b[39m[_StreamT] | \u001b[38;5;28;01mNone\u001b[39;00m = \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m   1245\u001b[39m ) -> ResponseT | _StreamT:\n\u001b[32m   1246\u001b[39m     opts = FinalRequestOptions.construct(\n\u001b[32m   1247\u001b[39m         method=\u001b[33m\"\u001b[39m\u001b[33mpost\u001b[39m\u001b[33m\"\u001b[39m, url=path, json_data=body, files=to_httpx_files(files), **options\n\u001b[32m   1248\u001b[39m     )\n\u001b[32m-> \u001b[39m\u001b[32m1249\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(ResponseT, \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mopts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/lib/python3.13/site-packages/openai/_base_client.py:1037\u001b[39m, in \u001b[36mSyncAPIClient.request\u001b[39m\u001b[34m(self, cast_to, options, stream, stream_cls)\u001b[39m\n\u001b[32m   1034\u001b[39m             err.response.read()\n\u001b[32m   1036\u001b[39m         log.debug(\u001b[33m\"\u001b[39m\u001b[33mRe-raising status error\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m-> \u001b[39m\u001b[32m1037\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m._make_status_error_from_response(err.response) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1039\u001b[39m     \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[32m   1041\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m response \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[33m\"\u001b[39m\u001b[33mcould not resolve response (should never happen)\u001b[39m\u001b[33m\"\u001b[39m\n",
      "\u001b[31mRateLimitError\u001b[39m: Error code: 429 - {'error': {'code': 'RateLimitReached', 'message': 'Rate limit of 50 per 86400s exceeded for UserByModelByDay. Please wait 43414 seconds before retrying.', 'details': 'Rate limit of 50 per 86400s exceeded for UserByModelByDay. Please wait 43414 seconds before retrying.'}}"
     ]
    }
   ],
   "source": [
    "import chat_assistant\n",
    "\n",
    "chat = chat_assistant.ChatAssistant(\n",
    "    tools=MCPTools(mcp_client=our_mcp_client),\n",
    "    developer_prompt=developer_prompt,\n",
    "    chat_interface=chat_assistant.ChatInterface(),\n",
    "    client=client\n",
    ")\n",
    "\n",
    "chat.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92b30f3c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ffd2ddf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
